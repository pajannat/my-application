{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‚è€ƒ  \n",
    "https://notebooks.githubusercontent.com/view/ipynb?browser=unknown_browser&color_mode=auto&commit=b34f8964fe3cdbbf0dd527dc381926721538ac8c&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d617473756f6c61622d6564752f646c3475732f623334663839363466653363646262663064643532376463333831393236373231353338616338632f6c6573736f6e342f6c6573736f6e345f736563325f65786572636973652e6970796e62&logged_in=false&nwo=matsuolab-edu%2Fdl4us&path=lesson4%2Flesson4_sec2_exercise.ipynb&platform=unknown_platform&repository_id=179020903&repository_type=Repository&version=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®Ÿè£…â‘ \n",
    "LSTMã‚’ä½¿ã£ãŸSeq2Seqãƒ¢ãƒ‡ãƒ«ã§ æ–‡ç« â†’ãºã“ã‚‰ãƒã‚¨ãƒ  å¤‰æ›ã‚’è¡Œã†ã€‚  \n",
    "ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€train.sentenceã¨train.poemã®ä¸­èº«ã¯æ¬¡ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™.  \n",
    "train.sentenceã®ä¸­èº« (ãƒã‚¨ãƒ ã®è¦ç´„)\n",
    "\n",
    "å‡„ã„ã¨å™‚ã®IDã‚µãƒã‚‚è¦‹ãŸã„ã—ã€ä»Šå¹´ã‚‚ã‚ã‚‹ã‚“ã ãƒ¼ã€‚ã‚ã¨ã€å¤ç¥­ã‚Šã®äºˆç¿’ã‚‚ã—ãªã„ã¨ï¼ï¼ˆç¬‘  \n",
    "ï¸™\n",
    "\n",
    "train.poemã®ä¸­èº«(è¦ç´„å‰ã®ãƒã‚¨ãƒ )\n",
    "\n",
    "ä»Šæ—¥ã¯ä¹…ã—ã¶ã‚Šã®ãƒã‚¤ã‚¯ãƒ©â£  å‡„ã„ã¨å™‚ã®IDé¯–ã‚‚è¦‹ã¦ã¿ãŸã„ã— ä»Šå¹´ã‚‚ã‚ã‚Šã¾ã™ï¼å¤ç¥­ã‚Šã®ä¸‹æº–å‚™ã‚‚ã—ãªã‘ã‚Œã°ï¼  æ¬²å¼µã‚Šãºã“ãƒ¼ã‚‰ãºã“ãŠãŠãŠãŠãŠãŠâœ¨âœ¨  \n",
    "ï¸™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„\n",
    "ã¾ãšã¯ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã§ã™ã€‚\n",
    "èª­ã¿è¾¼ã‚€éš›ã€æ–‡é ­ã‚’è¡¨ã™ä»®æƒ³å˜èªï¼ˆBOS, Beginning Of Sentenceï¼‰ã¨ã—ã¦\\<s>ã€æ–‡æœ«ã‚’è¡¨ã™ä»®æƒ³å˜èªï¼ˆEOS, End Of Sentenceï¼‰ã¨ã—ã¦<\\s>ã‚’ä»˜åŠ ã—ã¾ã™ã€‚\n",
    "ã¾ãŸã€BOS, EOSã‚’ã¤ã‘ãŸæ–‡ç« ã«ã¤ã„ã¦ã€Tokenizerã«ã‚ˆã£ã¦æ•°å€¤åŒ–ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "æœ€å¾Œã«ã€ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã€å„ç³»åˆ—ã®é•·ã•ã‚’ãã‚ãˆã¦ãŠãã¾ã™ã€‚ã“ã‚Œã«ã¯keras.preprocessing.sequence.pad_sequencesã‚’ç”¨ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):\n",
    "    tokenizer = Tokenizer(filters=\"\")\n",
    "    whole_texts = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "# èª­ã¿è¾¼ã¿ï¼†Tokenizerã«ã‚ˆã‚‹æ•°å€¤åŒ–\n",
    "x_train, tokenizer_en = load_data('./data/train.sentence.txt')\n",
    "y_train, tokenizer_ja = load_data('./data/train.poem.txt')\n",
    "\n",
    "en_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "x_train = pad_sequences(x_train, padding='post')\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "\n",
    "seqX_len = len(x_train[0])\n",
    "seqY_len = len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
    "ã“ã“ã§ã¯ã€LSTMã‚’ä½¿ç”¨ã—ã¦Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚  \n",
    "Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã¯mask_zero=Trueã‚’å¼•æ•°ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ä¸Šå…ˆç¨‹ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’ç„¡è¦–ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚  \n",
    "ã¾ãŸã€Recurrentãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å¯¾ã™ã‚‹return_state=Trueã‚„return_sequences=Trueã®æŒ‡å®šã‚’LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç”Ÿæˆæ™‚ã«è¡Œã£ã¦ã„ã¾ã™ã€‚  \n",
    "ãªãŠã€Functional APIã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "\n",
    "## ç¬¦å·åŒ–å™¨\n",
    "# Inputãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆè¿”ã‚Šå€¤ã¨ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã‚’å—ã‘å–ã‚‹ï¼‰\n",
    "encoder_inputs = Input(shape=(seqX_len,))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å±¤æ§‹æˆï¼ˆæ‰‹å‰ã®å±¤ã®è¿”ã‚Šå€¤ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã€æ¬¡ã®æ¥ç¶šã—ãŸã„å±¤ã«åˆ¥é€”å¼•æ•°ã¨ã—ã¦ä¸ãˆã‚‹ï¼‰\n",
    "# Inputãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ¥ç¶šï¼ˆ+Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼‰\n",
    "encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs) # shape: (seqX_len,)->(seqX_len, emb_dim)\n",
    "# Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ¥ç¶šï¼ˆ+LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼‰\n",
    "_, *encoder_states = LSTM(hid_dim, return_state=True)(encoder_embedded)  # shape: (seqX_len, emb_dim)->(hid_dim, )\n",
    "# ã“ã®LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã«é–¢ã—ã¦ã¯ä¸‹è¨˜ã«è£œè¶³ã‚ã‚Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## å¾©å·åŒ–å™¨\n",
    "# Inputãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆè¿”ã‚Šå€¤ã¨ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã‚’å—ã‘å–ã‚‹ï¼‰\n",
    "decoder_inputs = Input(shape=(seqY_len,))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å±¤æ§‹æˆï¼ˆæ‰‹å‰ã®å±¤ã®è¿”ã‚Šå€¤ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã€æ¬¡ã®æ¥ç¶šã—ãŸã„å±¤ã«åˆ¥é€”å¼•æ•°ã¨ã—ã¦ä¸ãˆã‚‹ï¼‰\n",
    "# Inputãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ¥ç¶š\n",
    "decoder_embedding = Embedding(ja_vocab_size, emb_dim) # å¾Œã§å‚ç…§ã—ãŸã„ã®ã§ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼è‡ªä½“ã‚’å¤‰æ•°åŒ–\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)  # shape: (seqY_len,)->(seqY_len, emb_dim)\n",
    "# Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ¥ç¶šï¼ˆencoder_statesã‚’åˆæœŸçŠ¶æ…‹ã¨ã—ã¦æŒ‡å®šï¼‰\n",
    "decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True) # å¾Œã§å‚ç…§ã—ãŸã„ã®ã§ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼è‡ªä½“ã‚’å¤‰æ•°åŒ–\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states) # shape: (seqY_len, emb_dim)->(seqY_len, hid_dim)\n",
    "# LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨Denseãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ¥ç¶š\n",
    "decoder_dense = Dense(ja_vocab_size, activation='softmax') # å¾Œã§å‚ç…§ã—ãŸã„ã®ã§ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼è‡ªä½“ã‚’å¤‰æ•°åŒ–\n",
    "decoder_outputs = decoder_dense(decoder_outputs) # shape: (seqY_len, hid_dim)->(seqY_len, ja_vocab_size)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆå…¥åŠ›ã¯ç¬¦å·åŒ–å™¨ï¼†å¾©å·åŒ–å™¨ã€å‡ºåŠ›ã¯å¾©å·åŒ–å™¨ã®ã¿ï¼‰\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "# ä»Šå›ã¯ã€sparse_categorical_crossentropyï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã—ã¦one_hotè¡¨ç¾ã®ãƒ™ã‚¯ãƒˆãƒ«ã§ãªãæ•°å€¤ã‚’å—ã‘å–ã‚‹categorical_crossentropyï¼‰ã‚’ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
    "ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ™‚ã«ã¯ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦1æ™‚ç‚¹å…ˆã®å˜èªã‚’ç¤ºã™ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¾ã™ã€‚(train_target)  \n",
    "å­¦ç¿’æ™‚ã«ã¯Decoderã®å…¥åŠ›ã«æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¾ã™ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 - 7s - loss: 3.9871 - val_loss: 3.6470 - 7s/epoch - 213ms/step\n",
      "Epoch 2/20\n",
      "31/31 - 1s - loss: 3.3459 - val_loss: 3.6959 - 1s/epoch - 35ms/step\n",
      "Epoch 3/20\n",
      "31/31 - 1s - loss: 3.1461 - val_loss: 3.9964 - 1s/epoch - 36ms/step\n",
      "Epoch 4/20\n",
      "31/31 - 1s - loss: 3.0424 - val_loss: 4.2559 - 1s/epoch - 35ms/step\n",
      "Epoch 5/20\n",
      "31/31 - 1s - loss: 2.9586 - val_loss: 4.2383 - 1s/epoch - 36ms/step\n",
      "Epoch 6/20\n",
      "31/31 - 1s - loss: 2.8808 - val_loss: 4.7319 - 1s/epoch - 35ms/step\n",
      "Epoch 7/20\n",
      "31/31 - 1s - loss: 2.7851 - val_loss: 4.5280 - 1s/epoch - 35ms/step\n",
      "Epoch 8/20\n",
      "31/31 - 1s - loss: 2.6976 - val_loss: 5.4353 - 1s/epoch - 36ms/step\n",
      "Epoch 9/20\n",
      "31/31 - 1s - loss: 2.6166 - val_loss: 5.5986 - 1s/epoch - 35ms/step\n",
      "Epoch 10/20\n",
      "31/31 - 1s - loss: 2.5466 - val_loss: 5.7276 - 1s/epoch - 36ms/step\n",
      "Epoch 11/20\n",
      "31/31 - 1s - loss: 2.4453 - val_loss: 5.9013 - 1s/epoch - 35ms/step\n",
      "Epoch 12/20\n",
      "31/31 - 1s - loss: 2.3765 - val_loss: 6.4508 - 1s/epoch - 37ms/step\n",
      "Epoch 13/20\n",
      "31/31 - 1s - loss: 2.3026 - val_loss: 6.6712 - 1s/epoch - 36ms/step\n",
      "Epoch 14/20\n",
      "31/31 - 1s - loss: 2.2166 - val_loss: 6.6342 - 1s/epoch - 36ms/step\n",
      "Epoch 15/20\n",
      "31/31 - 1s - loss: 2.1510 - val_loss: 6.7379 - 1s/epoch - 36ms/step\n",
      "Epoch 16/20\n",
      "31/31 - 1s - loss: 2.0833 - val_loss: 6.9686 - 1s/epoch - 36ms/step\n",
      "Epoch 17/20\n",
      "31/31 - 1s - loss: 2.0276 - val_loss: 6.9785 - 1s/epoch - 36ms/step\n",
      "Epoch 18/20\n",
      "31/31 - 1s - loss: 1.9489 - val_loss: 7.2480 - 1s/epoch - 35ms/step\n",
      "Epoch 19/20\n",
      "31/31 - 1s - loss: 1.8717 - val_loss: 7.1181 - 1s/epoch - 36ms/step\n",
      "Epoch 20/20\n",
      "31/31 - 1s - loss: 1.8055 - val_loss: 7.2027 - 1s/epoch - 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cab9205520>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=2, epochs=20, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆ\n",
    "å…ˆç¨‹å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ç³»åˆ—ã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚  \n",
    "ãã®ãŸã‚ã«ã¾ãšã¯å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿è¾¼ã‚“ã ã€ç³»åˆ—ç”Ÿæˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚  \n",
    "å­¦ç¿’æ™‚ã¨ã®é•ã„ã¯ã€å¾©å·åŒ–å™¨ãŒ1ã‚¹ãƒ†ãƒƒãƒ—ãšã¤å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã€çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®å…¥åŠ›ã¨å‡ºåŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã«åŠ ãˆã¦ã„ã‚‹ç‚¹ã§ã™ã€‚  \n",
    "(ã¾ãŸã€1ã‚¹ãƒ†ãƒƒãƒ—å‰ã®çŠ¶æ…‹ã‚’å¼•ãç¶™ã„ã§ç”ŸæˆãŒå¯èƒ½ã«ãªã‚‹ã‚ˆã†ã«ã€å¾©å·åŒ–å™¨ã®ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸçŠ¶æ…‹ã‚’æŒ‡å®šå¯èƒ½ã«ã—ã¦ã„ã¾ã™ã€‚)  \n",
    "ç”Ÿæˆã™ã‚‹éš›ã®Decoderã®å…¥åŠ›ã«ã¯ç¿»è¨³å…ˆã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¯ç”¨ã„ã¾ã›ã‚“ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ï¼ˆç”Ÿæˆç”¨ï¼‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "\n",
    "# ç¬¦å·åŒ–å™¨ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜æ§‹æˆã€å­¦ç¿’ã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ©ç”¨ï¼‰\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# å¾©å·åŒ–å™¨\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))] # decorder_lstmã®åˆæœŸçŠ¶æ…‹æŒ‡å®šç”¨(h_t, c_t)\n",
    "\n",
    "decoder_inputs = Input(shape=(1,))\n",
    "decoder_embedded = decoder_embedding(decoder_inputs) # å­¦ç¿’æ¸ˆã¿Embeddingãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ©ç”¨\n",
    "decoder_outputs, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs) # å­¦ç¿’æ¸ˆã¿LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ©ç”¨\n",
    "decoder_outputs = decoder_dense(decoder_outputs) # å­¦ç¿’æ¸ˆã¿Denseãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ©ç”¨\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆï¼ˆäºˆæ¸¬ï¼‰\n",
    "ç”Ÿæˆã§ã¯ã€æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã™ã‚‹ã®ã§æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚  \n",
    "ãã“ã§ã€ä»£ã‚ã‚Šã«å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã§äºˆæ¸¬ã—ãŸå˜èªã‚’å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å…¥åŠ›ã¨ã—ã¾ã™ã€‚  \n",
    "ãã—ã¦, ç³»åˆ—ã®çµ‚ã‚ã‚Šã‚’è¡¨ã™å˜èª (\\</s>) ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚ï¼ˆæœ€åˆã®å…¥åŠ›ã¯\\<s>ã‚’ä½¿ç”¨ã—ã¾ã™ï¼‰  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, bos_eos, max_output_length = 1000):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"ã«å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    output_seq= bos_eos[0][:]\n",
    "    \n",
    "    while True:\n",
    "        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "        \n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…ƒã®æ–‡: <s> ãƒšã‚³ãƒ©ã¯ãƒ­ã‚±ãƒ©ãƒ³ã§ä½•ã§ã‚‚ç ´å£Šã—ã¦ã—ã¾ã†ãƒ‘ãƒ¯ãƒ•ãƒ«ãªå¥³ã®å­ã§ã™ã€‚å½¼å¥³ã¯ä»²é–“ã«ãªã‚‹äººã‚’æ¢ã—ã¦ã„ã‚‹ã€‚å ±é…¬ã¯ã™ã¹ã¦å½¼å¥³ã®ã‚‚ã®ã§ã™ã€‚ </s>\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "ç”Ÿæˆæ–‡: <s> å‰å›è‹¦ã—ã‚ã‚‰ã‚ŒãŸã‚¦ã‚©ãƒ¼ãƒ‡ãƒ³â€¦ ä¼èª¬ã®æ–™ç†ã€Œé¾ã®é«­ã€ã‚’!!!!!! 1æœ¬â€¦2æœ¬â€¦4æœ¬â€¦8æœ¬â€¦16æœ¬â€¦ æœ€å¾Œã«ã¯1ä¸‡6åƒæœ¬ä»¥ä¸Šã®é«­ã‚’ä½œã‚‹ãºã“ğŸ’ªâœ¨ æ–°äººé¾ã®é«­è·äººãºã“ãƒ¼ã‚‰ã®æŠ€ã‚’ã”è¦§ã‚ã‚Œï¼ </s>\n",
      "æ­£è§£æ–‡: <s> ã’ãã†ã¾ï¼ã’ãã†ã¾ï¼ã‚„ã°ã„ãã‚‰ã„ç‰©è³‡ãŒã†ã¾ã™ãã‚‹ã‚‰ã—ã„ ã“ã‚Œã¯ã„ãã—ã‹ãªã„ãºã“ã ã‚ˆã­ï¼Ÿ å±é™ºï¼Ÿå±ãªã„ï¼ŸãŠå‰ã˜ã‚ƒç„¡ç†ã ã€‚ ãŠã„ãŠã„ã€ä½•è¨€ã£ã¦ã‚“ã ã„ã€‚ ãºã“ãƒ¼ã‚‰ã®å³è…•ã¿ãˆã­ãˆã®ã‹ã€‚ ã“ã®ãƒ­ã‚±ãƒ©ãƒ³ã§ã™ã¹ã¦ã‚’ç ´å£Šã—ã¦ã‚„ã‚‹ã‚ˆã€‚ ã¨ã‚‚ã«ã¤ã„ã¦ãã‚‹ã‚„ã¤ãŒã„ã‚‹ãªã‚‰ã¤ã„ã¦ã“ã„ï¼ å ±é…¬ã¯ã™ã¹ã¦ãºã“ã‚‰ã®ã‚‚ã® </s>\n"
     ]
    }
   ],
   "source": [
    "detokenizer_en = dict(map(reversed, tokenizer_en.word_index.items()))\n",
    "detokenizer_ja = dict(map(reversed, tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no = 8\n",
    "input_seq = pad_sequences([x_test[text_no]], seqX_len, padding='post')\n",
    "bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "\n",
    "print('å…ƒã®æ–‡:', ' '.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('ç”Ÿæˆæ–‡:', ' '.join([detokenizer_ja[i] for i in decode_sequence(input_seq, bos_eos)]))\n",
    "print('æ­£è§£æ–‡:', ' '.join([detokenizer_ja[i] for i in y_test[text_no]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5da9aad921784b8644505a50d90de01e1bbce0d7772509339b5714d1cab40d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
